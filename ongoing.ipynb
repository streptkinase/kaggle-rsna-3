{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo mount -o discard,defaults /dev/sdb /mnt/disks/mount_point"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sudo chmod a+w /mnt/disks/mount_point/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting albumentations\n",
      "  Downloading https://files.pythonhosted.org/packages/b7/64/1bf10fa4a6bcaee1129d17ecea724443cfd79878b5e16f7b37a314fffb6c/albumentations-0.4.0.tar.gz (95kB)\n",
      "\u001b[K    100% |████████████████████████████████| 102kB 3.2MB/s a 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.11.1 in /usr/local/lib/python3.5/site-packages (from albumentations)\n",
      "Requirement already satisfied: scipy in /usr/local/lib/python3.5/site-packages (from albumentations)\n",
      "Collecting imgaug<0.2.7,>=0.2.5 (from albumentations)\n",
      "  Downloading https://files.pythonhosted.org/packages/ad/2e/748dbb7bb52ec8667098bae9b585f448569ae520031932687761165419a2/imgaug-0.2.6.tar.gz (631kB)\n",
      "\u001b[K    100% |████████████████████████████████| 634kB 2.1MB/s ta 0:00:011\n",
      "\u001b[?25hRequirement already satisfied: PyYAML in /usr/local/lib/python3.5/dist-packages (from albumentations)\n",
      "Requirement already satisfied: opencv-python>=4.1.1 in /usr/local/lib/python3.5/dist-packages (from albumentations)\n",
      "Requirement already satisfied: tbb4py in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->albumentations)\n",
      "Requirement already satisfied: mkl-random in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->albumentations)\n",
      "Requirement already satisfied: icc-rt in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->albumentations)\n",
      "Requirement already satisfied: mkl-fft in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->albumentations)\n",
      "Requirement already satisfied: mkl in /usr/local/lib/python3.5/dist-packages (from numpy>=1.11.1->albumentations)\n",
      "Requirement already satisfied: intel-numpy in /usr/local/lib/python3.5/dist-packages (from scipy->albumentations)\n",
      "Requirement already satisfied: scikit-image>=0.11.0 in /usr/local/lib/python3.5/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: six in /usr/local/lib/python3.5/dist-packages (from imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: tbb==2019.* in /usr/local/lib/python3.5/dist-packages (from tbb4py->numpy>=1.11.1->albumentations)\n",
      "Requirement already satisfied: intel-openmp in /usr/local/lib/python3.5/dist-packages (from icc-rt->numpy>=1.11.1->albumentations)\n",
      "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.5/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.5/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Collecting pillow>=4.3.0 (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "  Downloading https://files.pythonhosted.org/packages/a8/34/086feba718f629fc51f27f993fa72e210c1c25077cba2f9e9a2dcfa23a7f/Pillow-6.2.0-cp35-cp35m-manylinux1_x86_64.whl (2.1MB)\n",
      "\u001b[K    100% |████████████████████████████████| 2.1MB 804kB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: PyWavelets>=0.4.0 in /usr/local/lib/python3.5/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: imageio>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.5/dist-packages (from networkx>=2.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.5/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.5/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.5/dist-packages (from kiwisolver>=1.0.1->matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.11.0->imgaug<0.2.7,>=0.2.5->albumentations)\n",
      "Building wheels for collected packages: albumentations, imgaug\n",
      "  Running setup.py bdist_wheel for albumentations ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/94/7d/3b/82e3372b2f6fe2b12635df47fd3ff18bec8df488db0f5475e6\n",
      "  Running setup.py bdist_wheel for imgaug ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/97/ec/48/0d25896c417b715af6236dbcef8f0bed136a1a5e52972fc6d0\n",
      "Successfully built albumentations imgaug\n",
      "Installing collected packages: imgaug, albumentations, pillow\n",
      "Successfully installed albumentations-0.4.0 imgaug-0.2.6 pillow-6.2.0\n"
     ]
    }
   ],
   "source": [
    "!pip3 install albumentations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting efficientnet-pytorch\n",
      "  Downloading https://files.pythonhosted.org/packages/82/18/1c4d61eea11b78235ce270a528e099b19af2f1026aadf45e9c645cd75e2f/efficientnet_pytorch-0.5.1.tar.gz\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.7/site-packages (from efficientnet-pytorch) (1.2.0)\n",
      "Requirement already satisfied: numpy in /opt/anaconda3/lib/python3.7/site-packages (from torch->efficientnet-pytorch) (1.17.2)\n",
      "Building wheels for collected packages: efficientnet-pytorch\n",
      "  Building wheel for efficientnet-pytorch (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Stored in directory: /home/jupyter/.cache/pip/wheels/52/0c/8e/58d91f29f0f8cbb45231a145312939a5423dc82c61cc731361\n",
      "Successfully built efficientnet-pytorch\n",
      "Installing collected packages: efficientnet-pytorch\n",
      "Successfully installed efficientnet-pytorch-0.5.1\n"
     ]
    }
   ],
   "source": [
    "!pip install efficientnet-pytorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "\n",
    "sys.path.append('/home/jupyter/.local/lib/python3.5/site-packages')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.7/site-packages/tqdm/autonotebook.py:17: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
      "  \" (e.g. in jupyter console)\", TqdmExperimentalWarning)\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function, with_statement, division\n",
    "import copy\n",
    "import os\n",
    "import torch\n",
    "from tqdm.autonotebook import tqdm\n",
    "from torch.optim.lr_scheduler import _LRScheduler\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "class LRFinder(object):\n",
    "    \"\"\"Learning rate range test.\n",
    "    The learning rate range test increases the learning rate in a pre-training run\n",
    "    between two boundaries in a linear or exponential manner. It provides valuable\n",
    "    information on how well the network can be trained over a range of learning rates\n",
    "    and what is the optimal learning rate.\n",
    "    Arguments:\n",
    "        model (torch.nn.Module): wrapped model.\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer where the defined learning\n",
    "            is assumed to be the lower boundary of the range test.\n",
    "        criterion (torch.nn.Module): wrapped loss function.\n",
    "        device (str or torch.device, optional): a string (\"cpu\" or \"cuda\") with an\n",
    "            optional ordinal for the device type (e.g. \"cuda:X\", where is the ordinal).\n",
    "            Alternatively, can be an object representing the device on which the\n",
    "            computation will take place. Default: None, uses the same device as `model`.\n",
    "        memory_cache (boolean): if this flag is set to True, `state_dict` of model and\n",
    "            optimizer will be cached in memory. Otherwise, they will be saved to files\n",
    "            under the `cache_dir`.\n",
    "        cache_dir (string): path for storing temporary files. If no path is specified,\n",
    "            system-wide temporary directory is used.\n",
    "            Notice that this parameter will be ignored if `memory_cache` is True.\n",
    "    Example:\n",
    "        >>> lr_finder = LRFinder(net, optimizer, criterion, device=\"cuda\")\n",
    "        >>> lr_finder.range_test(dataloader, end_lr=100, num_iter=100)\n",
    "    Cyclical Learning Rates for Training Neural Networks: https://arxiv.org/abs/1506.01186\n",
    "    fastai/lr_find: https://github.com/fastai/fastai\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, model, optimizer, criterion, device=None, memory_cache=True, cache_dir=None):\n",
    "        self.model = model\n",
    "        self.optimizer = optimizer\n",
    "        self.criterion = criterion\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "        self.memory_cache = memory_cache\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        # Save the original state of the model and optimizer so they can be restored if\n",
    "        # needed\n",
    "        self.model_device = next(self.model.parameters()).device\n",
    "        self.state_cacher = StateCacher(memory_cache, cache_dir=cache_dir)\n",
    "        self.state_cacher.store('model', self.model.state_dict())\n",
    "        self.state_cacher.store('optimizer', self.optimizer.state_dict())\n",
    "\n",
    "        # If device is None, use the same as the model\n",
    "        if device:\n",
    "            self.device = device\n",
    "        else:\n",
    "            self.device = self.model_device\n",
    "\n",
    "    def reset(self):\n",
    "        \"\"\"Restores the model and optimizer to their initial states.\"\"\"\n",
    "        self.model.load_state_dict(self.state_cacher.retrieve('model'))\n",
    "        self.optimizer.load_state_dict(self.state_cacher.retrieve('optimizer'))\n",
    "        self.model.to(self.model_device)\n",
    "\n",
    "    def range_test(\n",
    "        self,\n",
    "        train_loader,\n",
    "        val_loader=None,\n",
    "        end_lr=10,\n",
    "        num_iter=100,\n",
    "        step_mode=\"exp\",\n",
    "        smooth_f=0.05,\n",
    "        diverge_th=5,\n",
    "    ):\n",
    "        \"\"\"Performs the learning rate range test.\n",
    "        Arguments:\n",
    "            train_loader (torch.utils.data.DataLoader): the training set data laoder.\n",
    "            val_loader (torch.utils.data.DataLoader, optional): if `None` the range test\n",
    "                will only use the training loss. When given a data loader, the model is\n",
    "                evaluated after each iteration on that dataset and the evaluation loss\n",
    "                is used. Note that in this mode the test takes significantly longer but\n",
    "                generally produces more precise results. Default: None.\n",
    "            end_lr (float, optional): the maximum learning rate to test. Default: 10.\n",
    "            num_iter (int, optional): the number of iterations over which the test\n",
    "                occurs. Default: 100.\n",
    "            step_mode (str, optional): one of the available learning rate policies,\n",
    "                linear or exponential (\"linear\", \"exp\"). Default: \"exp\".\n",
    "            smooth_f (float, optional): the loss smoothing factor within the [0, 1[\n",
    "                interval. Disabled if set to 0, otherwise the loss is smoothed using\n",
    "                exponential smoothing. Default: 0.05.\n",
    "            diverge_th (int, optional): the test is stopped when the loss surpasses the\n",
    "                threshold:  diverge_th * best_loss. Default: 5.\n",
    "        \"\"\"\n",
    "        # Reset test results\n",
    "        self.history = {\"lr\": [], \"loss\": []}\n",
    "        self.best_loss = None\n",
    "\n",
    "        # Move the model to the proper device\n",
    "        self.model.to(self.device)\n",
    "\n",
    "        # Initialize the proper learning rate policy\n",
    "        if step_mode.lower() == \"exp\":\n",
    "            lr_schedule = ExponentialLR(self.optimizer, end_lr, num_iter)\n",
    "        elif step_mode.lower() == \"linear\":\n",
    "            lr_schedule = LinearLR(self.optimizer, end_lr, num_iter)\n",
    "        else:\n",
    "            raise ValueError(\"expected one of (exp, linear), got {}\".format(step_mode))\n",
    "\n",
    "        if smooth_f < 0 or smooth_f >= 1:\n",
    "            raise ValueError(\"smooth_f is outside the range [0, 1[\")\n",
    "\n",
    "        # Create an iterator to get data batch by batch\n",
    "        iterator = iter(train_loader)\n",
    "        for iteration in tqdm(range(num_iter)):\n",
    "            # Get a new set of inputs and labels\n",
    "            try:\n",
    "                inputs, labels = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(train_loader)\n",
    "                inputs, labels = next(iterator)\n",
    "\n",
    "            # Train on batch and retrieve loss\n",
    "            loss = self._train_batch(inputs, labels)\n",
    "            if val_loader:\n",
    "                loss = self._validate(val_loader)\n",
    "\n",
    "            # Update the learning rate\n",
    "            lr_schedule.step()\n",
    "            self.history[\"lr\"].append(lr_schedule.get_lr()[0])\n",
    "\n",
    "            # Track the best loss and smooth it if smooth_f is specified\n",
    "            if iteration == 0:\n",
    "                self.best_loss = loss\n",
    "            else:\n",
    "                if smooth_f > 0:\n",
    "                    loss = smooth_f * loss + (1 - smooth_f) * self.history[\"loss\"][-1]\n",
    "                if loss < self.best_loss:\n",
    "                    self.best_loss = loss\n",
    "\n",
    "            # Check if the loss has diverged; if it has, stop the test\n",
    "            self.history[\"loss\"].append(loss)\n",
    "            if loss > diverge_th * self.best_loss:\n",
    "                print(\"Stopping early, the loss has diverged\")\n",
    "                break\n",
    "\n",
    "        print(\"Learning rate search finished. See the graph with {finder_name}.plot()\")\n",
    "\n",
    "    def _train_batch(self, inputs, labels):\n",
    "        # Set model to training mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Move data to the correct device\n",
    "        inputs = inputs.to(self.device, dtype=torch.float)\n",
    "        labels = labels.to(self.device, dtype=torch.float)\n",
    "\n",
    "        # Forward pass\n",
    "        self.optimizer.zero_grad()\n",
    "        outputs = self.model(inputs)\n",
    "        loss = self.criterion(outputs, labels)\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        return loss.item()\n",
    "\n",
    "    def _validate(self, dataloader):\n",
    "        # Set model to evaluation mode and disable gradient computation\n",
    "        running_loss = 0\n",
    "        self.model.eval()\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in dataloader:\n",
    "                # Move data to the correct device\n",
    "                inputs = inputs.to(self.device)\n",
    "                labels = labels.to(self.device)\n",
    "\n",
    "                # Forward pass and loss computation\n",
    "                outputs = self.model(inputs)\n",
    "                loss = self.criterion(outputs, labels)\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "\n",
    "        return running_loss / len(dataloader.dataset)\n",
    "\n",
    "    def plot(self, skip_start=10, skip_end=5, log_lr=True):\n",
    "        \"\"\"Plots the learning rate range test.\n",
    "        Arguments:\n",
    "            skip_start (int, optional): number of batches to trim from the start.\n",
    "                Default: 10.\n",
    "            skip_end (int, optional): number of batches to trim from the start.\n",
    "                Default: 5.\n",
    "            log_lr (bool, optional): True to plot the learning rate in a logarithmic\n",
    "                scale; otherwise, plotted in a linear scale. Default: True.\n",
    "        \"\"\"\n",
    "\n",
    "        if skip_start < 0:\n",
    "            raise ValueError(\"skip_start cannot be negative\")\n",
    "        if skip_end < 0:\n",
    "            raise ValueError(\"skip_end cannot be negative\")\n",
    "\n",
    "        # Get the data to plot from the history dictionary. Also, handle skip_end=0\n",
    "        # properly so the behaviour is the expected\n",
    "        lrs = self.history[\"lr\"]\n",
    "        losses = self.history[\"loss\"]\n",
    "        if skip_end == 0:\n",
    "            lrs = lrs[skip_start:]\n",
    "            losses = losses[skip_start:]\n",
    "        else:\n",
    "            lrs = lrs[skip_start:-skip_end]\n",
    "            losses = losses[skip_start:-skip_end]\n",
    "\n",
    "        # Plot loss as a function of the learning rate\n",
    "        plt.plot(lrs, losses)\n",
    "        if log_lr:\n",
    "            plt.xscale(\"log\")\n",
    "        plt.xlabel(\"Learning rate\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "class LinearLR(_LRScheduler):\n",
    "    \"\"\"Linearly increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float, optional): the initial learning rate which is the lower\n",
    "            boundary of the test. Default: 10.\n",
    "        num_iter (int, optional): the number of iterations over which the test\n",
    "            occurs. Default: 100.\n",
    "        last_epoch (int): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(LinearLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr + r * (self.end_lr - base_lr) for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class ExponentialLR(_LRScheduler):\n",
    "    \"\"\"Exponentially increases the learning rate between two boundaries over a number of\n",
    "    iterations.\n",
    "    Arguments:\n",
    "        optimizer (torch.optim.Optimizer): wrapped optimizer.\n",
    "        end_lr (float, optional): the initial learning rate which is the lower\n",
    "            boundary of the test. Default: 10.\n",
    "        num_iter (int, optional): the number of iterations over which the test\n",
    "            occurs. Default: 100.\n",
    "        last_epoch (int): the index of last epoch. Default: -1.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, optimizer, end_lr, num_iter, last_epoch=-1):\n",
    "        self.end_lr = end_lr\n",
    "        self.num_iter = num_iter\n",
    "        super(ExponentialLR, self).__init__(optimizer, last_epoch)\n",
    "\n",
    "    def get_lr(self):\n",
    "        curr_iter = self.last_epoch + 1\n",
    "        r = curr_iter / self.num_iter\n",
    "        return [base_lr * (self.end_lr / base_lr) ** r for base_lr in self.base_lrs]\n",
    "\n",
    "\n",
    "class StateCacher(object):\n",
    "    def __init__(self, in_memory, cache_dir=None):\n",
    "        self.in_memory = in_memory\n",
    "        self.cache_dir = cache_dir\n",
    "\n",
    "        if self.cache_dir is None:\n",
    "            import tempfile\n",
    "            self.cache_dir = tempfile.gettempdir()\n",
    "        else:\n",
    "            if not os.path.isdir(self.cache_dir):\n",
    "                raise ValueError('Given `cache_dir` is not a valid directory.')\n",
    "\n",
    "        self.cached = {}\n",
    "\n",
    "    def store(self, key, state_dict):\n",
    "        if self.in_memory:\n",
    "            self.cached.update({key: copy.deepcopy(state_dict)})\n",
    "        else:\n",
    "            fn = os.path.join(self.cache_dir, 'state_{}_{}.pt'.format(key, id(self)))\n",
    "            self.cached.update({key: fn})\n",
    "            torch.save(state_dict, fn)\n",
    "\n",
    "    def retrieve(self, key):\n",
    "        if key not in self.cached:\n",
    "            raise KeyError('Target {} was not cached.'.format(key))\n",
    "\n",
    "        if self.in_memory:\n",
    "            return self.cached.get(key)\n",
    "        else:\n",
    "            fn = self.cached.get(key)\n",
    "            if not os.path.exists(fn):\n",
    "                raise RuntimeError('Failed to load state in {}. File does not exist anymore.'.format(fn))\n",
    "            state_dict = torch.load(fn, map_location=lambda storage, location: storage)\n",
    "            return state_dict\n",
    "\n",
    "    def __del__(self):\n",
    "        \"\"\"Check whether there are unused cached files existing in `cache_dir` before\n",
    "        this instance being destroyed.\"\"\"\n",
    "        if self.in_memory:\n",
    "            return\n",
    "\n",
    "        for k in self.cached:\n",
    "            if os.path.exists(self.cached[k]):\n",
    "                os.remove(self.cached[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'albumentations'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-a9f877436236>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptim\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mCompose\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mShiftScaleRotate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mResize\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mCenterCrop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mToFloat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mPadIfNeeded\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0malbumentations\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpytorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mToTensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'albumentations'"
     ]
    }
   ],
   "source": [
    "dir_csv = '/mnt/disks/mount_point'\n",
    "dir_train_img = '/mnt/disks/mount_point/stage_1_train_images_jpg'\n",
    "dir_test_img = '/mnt/disks/mount_point/stage_1_test_images_jpg'\n",
    "\n",
    "n_classes = 6\n",
    "n_epochs = 1\n",
    "batch_size = 11\n",
    "\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import glob\n",
    "#import pydicom\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from efficientnet_pytorch import EfficientNet\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from albumentations import Compose, ShiftScaleRotate, Resize, CenterCrop, ToFloat, PadIfNeeded\n",
    "from albumentations.pytorch import ToTensor\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm_notebook as tqdm\n",
    "#from torch_lr_finder import LRFinder\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class IntracranialDataset(Dataset):\n",
    "\n",
    "    def __init__(self, csv_file, path, labels, transform=None):\n",
    "\n",
    "        self.path = path\n",
    "        self.data = pd.read_csv(csv_file)\n",
    "        self.transform = transform\n",
    "        self.labels = labels\n",
    "\n",
    "    def __len__(self):\n",
    "\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_name = os.path.join(self.path, self.data.loc[idx, 'Image'] + '.jpg')\n",
    "        img = cv2.imread(img_name)\n",
    "\n",
    "        if self.transform:\n",
    "\n",
    "            augmented = self.transform(image=img)\n",
    "            img = augmented['image']\n",
    "\n",
    "        if self.labels:\n",
    "\n",
    "            labels = torch.tensor(\n",
    "                self.data.loc[idx, ['epidural', 'intraparenchymal', 'intraventricular', 'subarachnoid', 'subdural', 'any']])\n",
    "            #return {'image': img, 'labels': labels}\n",
    "            return (img, labels)\n",
    "\n",
    "        else:\n",
    "\n",
    "            #return {'image': img}\n",
    "            return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform_train = Compose([\n",
    "    PadIfNeeded(min_height=456, min_width=456, always_apply=True),\n",
    "    CenterCrop(456, 456, always_apply=True),\n",
    "    ToFloat(max_value=127, always_apply=True),\n",
    "    ShiftScaleRotate(),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "transform_valid = Compose([\n",
    "    Resize(456, 456),\n",
    "    ToFloat(max_value=127, always_apply=True),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "transform_test= Compose([\n",
    "    Resize(456, 456),\n",
    "    ToFloat(max_value=127, always_apply=True),\n",
    "    ToTensor()\n",
    "])\n",
    "\n",
    "train_dataset = IntracranialDataset(\n",
    "    csv_file='/mnt/disks/mount_point/train.csv', path='/mnt/disks/mount_point/stage_1_train_images_jpg', transform=transform_train, labels=True)\n",
    "\n",
    "valid_dataset = IntracranialDataset(\n",
    "    csv_file='/mnt/disks/mount_point/valid.csv', path='/mnt/disks/mount_point/stage_1_train_images_jpg', transform=transform_valid, labels=True)\n",
    "\n",
    "test_dataset = IntracranialDataset(\n",
    "    csv_file='/mnt/disks/mount_point/test.csv', path='/mnt/disks/mount_point/stage_1_test_images_jpg', transform=transform_test, labels=False)\n",
    "\n",
    "data_loader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "data_loader_valid = torch.utils.data.DataLoader(valid_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "data_loader_test = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b5\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda:0\")\n",
    "model = EfficientNet.from_pretrained('efficientnet-b5') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model._fc = torch.nn.Linear(2048, n_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "criterion = torch.nn.BCEWithLogitsLoss()\n",
    "plist = [{'params': model.parameters(), 'lr': 5e-6}]\n",
    "optimizer = optim.Adam(plist, lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aefc444674f34a658ee14d3e2bc45638",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping early, the loss has diverged\n",
      "Learning rate search finished. See the graph with {finder_name}.plot()\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEKCAYAAAAfGVI8AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dd3yV5f3/8dfnnGyygIQZpkDYQyJOEFtUqtYJClZbrXW0bn9W7bJqv1arVqtVq1grHSpVaSsqFhcucAAylJEQGRJG2GTnZFy/P3KgEQMEyMl9xvv5eJyH577Pfc79yW0479zXdd/XZc45REQkdvm8LkBERLylIBARiXEKAhGRGKcgEBGJcQoCEZEYpyAQEYlxcV4XcLCysrJcz549vS5DRCSiLFiwYKtzLrup1yIuCHr27Mn8+fO9LkNEJKKY2dp9vaamIRGRGKcgEBGJcQoCEZEYpyAQEYlxCgIRkRinIBARiXEKAhGRCPDyovWUVtWE5LMVBCIiYe7zol1cP20R0z5dF5LPVxCIiIS5J977krSkOCaN6haSz1cQiIiEsTVby3n9i41cdEwP0pLiQ7IPBYGISBh76oNVxPl8XHp8z5DtQ0EgIhKmtpRW8+KCIs4b2ZUOaUkh24+CQEQkTE2du5qaunouH907pPtREIiIhKGy6lr+/tFaxg/qRO/s1JDuS0EgIhKGpn36FSVVtVx14hEh35eCQEQkzARq6/nzB6s5tnd7hnXLDPn+FAQiImHm5UXr2VRSxZUnhrZvYDcFgYhIGKmvdzz5/ioGdE7nxH5NzizZ4hQEIiJh5L2CLRRuLuOqE3tjZq2yTwWBiEgYyS8uBWDcgI6ttk8FgYhIGKkI1AGQHO9vtX0qCEREwkhloJbkeD8+X+s0C4GCQEQkrFQE6khJaL2zAVAQiIiElcpAHckKAhGR2KUzAhGRGFceqCU5Ia5V96kgEBEJI5WBOlJa8YohUBCIiIQVNQ2JiMS4yhp1FouIxLSKQK3OCEREYllD05A6i0VEYpbuIxARiWGB2npq6110XTVkZuPNLN/MCs3stiZef8jMFgUfBWa2M5T1iIiEs8rdA8618hlByBqizMwPPAacDBQB88xshnNu2e5tnHM3Ntr+WmBEqOoREQl3FTW1AK3eRxDKvY0CCp1zqwDMbBpwFrBsH9tPBn4dqmIWrN3OnMJt+AzMDJ8Zfh/4zMhIjqd9agLt2yTSrk0C7VMTWv1/hIjI7iGoW/uqoVB+23UF1jVaLgKObmpDM+sB9ALeCVUx89bs4ME3C5q9fWpiHJ0zkuiUkUSXjGQ6BZ9npyaSnZZIVloiWakJJMa17v8wEYleUdc0BDQ1mLbbx7aTgJecc3VNfpDZFcAVAN27dz+kYq4c05vLR/em3jnq6h3OQb1z1NY5dlXWsK28mm1lAbaXB9haXs3mkmo27api465K8jeVsqWsGtdE9elJceS0TaFnVgo92rehR7uG//bKakPH9MRWm2pORCJfNJ4RFAHdGi3nABv2se0k4Op9fZBzbgowBSAvL29fYbJfZobfwI+xd4d8Rko83dun7Pf9gdp6tpRVs7W0mi2l1Wwta/jv5tJq1u2oYMXGUt5cVkxN3f/KS0uMo3eHVPpkp9KnQyr9OqYyNCeT7LTEQ/kRRCTKVQSir49gHtDXzHoB62n4sr9w743MLBdoC3wUwloOW0Kcj66ZyXTNTN7nNrV19WzcVcWabeWs3lpO4eYyvtxSxoeFW5j+WdGe7bpmJjO8eyYjumUyvFsmQ3MySYjTlbwisa4y2s4InHO1ZnYNMAvwA39xzi01s7uA+c65GcFNJwPTnGuq4SWyxPl9dGuXQrd2KYzum/2110qqaijYVMqidTtZuG4ni77ayWtLNgLQJsHP8X2yGJvbgbG52XTZT9iISPSKxqYhnHMzgZl7rbt9r+U7QllDuEhPiievZzvyerbbs25zaRWfrd3J+yu38F7+Ft5YVgxAv46pnNAnm6N6tmVkz7Z0SEvyqmwRaUUVNdHXWSwH0CEtifGDOzF+cCeccxRuLuPd/C28W7CZZz9Zy1/mrAagR/sU8nq04+he7Ribm02HdAWDSDSqjMI+AjkIZkbfjmn07ZjG5WN6E6it54sNu1iwZgfz1mxndv7mPf0MQ3My+Fb/Dowb0JFBXdJ1ZZJIlNjdNJTcykNMKAjCVEKcjyO7t+XI7m25fExvnHPkF5fy9vLNvL28mIffXskf3lpJp/SGs4rvDuvMiG5t8fkUCiKRqjJQR2KcD38r/ztWEEQIM6N/p3T6d0rn6pP6sLWsmtkrNvPmsmKe+/Qrps5dQ9fMZM4Y2pnvDuuiMwWRCOTF7GSgIIhYWamJTMzrxsS8bpRW1fDmsmJeXbKRpz9czZPvr6JPh1QuOro7547MIT0p3utyRaQZvJiLABQEUSEtKZ5zj8zh3CNz2FkR4PUvNjFt3jrueGUZ983K5+wRXbn4mB4M6Jzudakish+VNbWtfsUQKAiiTmZKApNHdWfyqO4sKdrJ3z9ay/QFRTz3yVcc1bMtFx3Tg/GDO2mMJJEwpKYhaXFDczK5f2Imvzh9AC/OL+Ifn6zl+mmLaNcmgYl5OXxvVI8DDq0hIq2nIlDX6lcMgYIgJmSmJHD5mN5cdkIvPizcyrOfrOXPH6xmyvurGNM3mx+e0IsxfbPUuSziscpAHVmpCa2+XwVBDPH5jDH9shnTL5uNuyqZ9uk6nv/0K37wl08Z0Dmdq07szelDOhPn17hHIl6oCNSSktD6Z+n6Fx+jOmckc+PJ/fjw1m9x34ShBGrruH7aIsY+8C5/nbtmz+BXItJ6vJi4HhQEMS8hzsf5ed1488YTeer7eXRMT+LXM5Yy7sH3+HT1dq/LE4kpFTXedBYrCARoaDY6eWBHpv/4OKZdcQxxfmPSlI94YFY+NXX1XpcnEhMqdEYg4eKY3u157brRTBiZw6OzC5nwxEes2VrudVkiUa2u3hGorSclvvW7bhUE0qTUxDjumzCMxy48ktVbyjjtkQ94Yf46omDaCJGw9L/ZyXRGIGHm9KGd+e8NYxiak8EtLy3h2ucXUlJV43VZIlHHq4nrQUEgzdAlM5lnf3QMPz01l9e/2MRpD3/AZ1/t8Loskaji1exkoCCQZvL7jKtP6sMLVx6LczDxiY94bHYh9fVqKhJpCQoCiRgje7Rl5vWjGT+oE/fPyufiv3zC5tIqr8sSiXiVNQ19BMkejD6qIJCDlpEcz6MXjuB35w1hwdodnPenuazWVUUih0VnBBJxzIwLjurOtCuOpby6jgl/msvnRbu8LkskYnk1TSUoCOQwDe+WyYtXHUtSvJ9JUz7iw5VbvS5JJCJV6oxAItkR2an86yfH0a1dCpdO/ZQZizd4XZJIxPlf05D6CCRCdUxP4p9XHsuIbm257vmFTJ2z2uuSRCLK7hvKdB+BRLSM5Hj+dtkoThnYkTteWcaj76zUncgizaSmIYkaSfF+Hv/ekZw7oisPvFHA7/6brzAQaYbyQB3xfiPeg/lANDGNtLg4v48HJg4jOcHPE+99SXl1LXeeOQifTzOgiexLZaDWkyuGQEEgIeLzGf939mBSE+N48v1VVATq+N15QzT7mcg+NExc781XsoJAQsbMuO07/WmTGMeDbxZQEajlD5OGkxjnzV89IuHMq0lpQH0EEmJmxnXf7suvzhjI619s4rKp8ymrrvW6LJGw49U0laAgkFZy2Qm9eGDiMD5atY3vPfUx28qqvS5JJKw0TFyvIJAoN2FkDk9eNJIVm0qZ+ORHFO2o8LokkbDRcEbgTWu9gkBa1biBHfnHj45mS2k1E/70EQXFpV6XJBIWKgJ1pHh01ZCCQFrdUT3b8cKVx1LvHBOf+EiD1Ymw+6ohBYHEkAGd05n+4+NITYzjqn8sYFelpr+U2FZZo85iiUHd2qXw6IUjKC6p4rbpS3QHssQ0dRZLzBrRve2euZCf/eQrr8sR8UR9vaOqpl6dxRK7Lh/dmzH9srnr1WUs31jidTkira6ypmHAuTY6I5BY5fMZD54/jIzkeK59fuGe4XhFYoWX01RCiIPAzMabWb6ZFZrZbfvY5nwzW2ZmS83suVDWI+ErKzWRP1wwnC+3lHHnjGVelyPSqnYPQR11TUNm5gceA74DDAQmm9nAvbbpC/wMON45Nwi4IVT1SPg7vk8WV4/twz/nr+PlReu9Lkek1VTUNJwFR+MZwSig0Dm3yjkXAKYBZ+21zeXAY865HQDOuc0hrEciwA3j+pLXoy2/+PcXfLVNdx5LbNgzcX0UBkFXYF2j5aLgusb6Af3MbI6ZfWxm40NYj0SAOL+PP0wajhlc/8+F1NbVe12SSMjtmZ0sCu8sbmoWkr0vFI8D+gJjgcnAn80s8xsfZHaFmc03s/lbtmxp8UIlvOS0TeGec4ew8KudPPL2Sq/LEQk5Lyeuh9AGQRHQrdFyDrChiW1eds7VOOdWA/k0BMPXOOemOOfynHN52dnZIStYwscZQ7swYWQOj84u5JNV27wuRySkvJy4HkIbBPOAvmbWy8wSgEnAjL22+Q9wEoCZZdHQVLQqhDVJBLnjzEF0b5fCjf9cxK4KDUEh0cvLieshhEHgnKsFrgFmAcuBF5xzS83sLjM7M7jZLGCbmS0DZgM/dc7pzz8BIDUxjocnjWBzaTU///fnGoJCopbX9xGEtEHKOTcTmLnXutsbPXfATcGHyDcM65bJ/zsll9/9dwUnLsjm/LxuB36TSITZfWdxNDYNibSIK8f05rgj2nPHjKWs3VbudTkiLa4iUIvfZyT4vflKVhBI2PP5jN+fPwy/GT/7l5qIJPrsnpTGrKmLLUNPQSARoXNGMred1p+5X27jxflFXpcj0qK8nLgeFAQSQSYf1Z1Rvdrxf68tY3NpldfliLQYL2cnAwWBRBCfz7j33CFU1dZzx4ylXpcj0mIqPJy4HhQEEmF6Z6dy/bf7MvPzTcxausnrckRaRGWNd7OTgYJAItAVY3ozoHM6v/rPF5rrWKKCmoZEDlK838fvzhvC1rJq7n19udfliBy2ykAdyR4NOAfNDAIzO8LMEoPPx5rZdU0NDifSWobmZPKj0b15/tN1zC3c6nU5IoclUs4IpgN1ZtYHeBroBWg2MfHUjeP60TurDdc+v5D1Oyu9LkfkkEVKZ3F9cOygc4A/OOduBDqHriyRA0tO8DPl+3kEauu5/K/zNdexRKzKQGR0FteY2WTgB8CrwXXxoSlJpPn6dEjlkQtHsHxTCTe/uFh3HUvEcc5RURMZTUOXAscCdzvnVptZL+AfoStLpPlOyu3Az77Tn5mfb+KP7xR6XY7IQamurcc57wacg2aOPuqcWwZcB2BmbYE059y9oSxM5GBcPro3KzaV8uCbBfTrmMr4wWq5lMhQ4fE0ldD8q4beNbN0M2sHLAaeMbMHQ1uaSPOZGb89ZwjDu2Vy4z8Xs2xDidcliTTL7r4tr6aphOY3DWU450qAc4FnnHMjgXGhK0vk4CXF+5ly8UjSk+P40V/nsXGXriSS8Ld7drJIGHQuzsw6A+fzv85ikbDTIT2Jp39wFKVVtVz89KfsKA94XZLIfnk9Oxk0PwjuomFayS+dc/PMrDewMnRliRy6wV0zeOoHeXy1vYJLps6jvFqXlUr4qoiUMwLn3IvOuaHOuR8Hl1c5584LbWkih+6Y3u15dPIIPi/ayVX/WEB1bZ3XJYk0qbImQvoIzCzHzP5tZpvNrNjMpptZTqiLEzkcpwzqxL3nDeWDlVu56YXF1NXrHgMJP+XVkdM09AwwA+gCdAVeCa4TCWvn53Xj56f157UlG7n95S90w5mEnT2dxR5ePtrcc5Fs51zjL/6pZnZDKAoSaWlXjDmC7eU1PPHel7Rvk8BNp+R6XZLIHv+7fDT8g2CrmV0EPB9cngxsC01JIi3v1vG57CgP8Mg7hbRtk8Clx/fyuiQRACpqdjcNeddH0Nw9/xB4FHgIcMBcGoadEIkIZsbd5wxmZ2WAO19ZRtuUBM4e0dXrskSoDNRhBknx3k0P09yrhr5yzp3pnMt2znVwzp1Nw81lIhEjzu/j4UkjOLZ3e25+cTGzV2z2uiSRhiGo4/2YmWc1HE4E3dRiVYi0kqR4P1O+P5L+ndP48bMLmL9mu9clSYzzelIaOLwg8C6+RA5DWlI8Uy8dRZeMZH44dR75m0q9LkliWGWg1tObyeDwgkDX4UnEykpN5G+XjSIhzs9PX1pMve4xEI9UBOpIifeuoxgOEARmVmpmJU08Smm4p0AkYuW0TeEXp/dnSdEuXlpQ5HU5EqMqa+rC+4zAOZfmnEtv4pHmnPM2wkRawNnDu3Jk90zum7WCkqoar8uRGBTpfQQiEc/MuPPMwWwrD/DIWxpHUVpfQxCEcdOQSCwYkpPBBXndmDp3DYWby7wuR2KM1xPXg4JABICbT80lOcHPXa8u03hE0qrUNCQSJrJSE7n+2315v2ALby/XjWbSeioDYd5ZLBJLfnBcT/p0SOU3ry3T/AXSKpxzVNTojEAkbMT7fdx+xkDWbqvgzx+s9rociQGBunrq6p06i0XCyZh+2ZwysCMPv72Sz4t2eV2ORLlwmIsAFAQi33DPuUPITk3kqn8sYHt5wOtyJIqFw8T1oCAQ+Yb2qYk8cdFItpRVc+3zn1FbV+91SRKlwmHieghxEJjZeDPLN7NCM7utidcvMbMtZrYo+PhRKOsRaa4hORncffZg5hRu4/5Z+fvcbvXW8j0zTIkcrMqA95PSQPMnpjloZuYHHgNOBoqAeWY2wzm3bK9N/+mcuyZUdYgcqol53VhctJMn31/F0JxMTh/aec9ri9ft5OG3V/LOis0c36c9f/vh0fh9GpBXDk44TFMJIQwCYBRQ6JxbBWBm04CzgL2DQCRs3X7GIJZvLOWnLy2mT4dUKgK1PPz2St7N30JmSjxnDuvCjMUbeOK9L7n6pD5elysRZvc0lV43DYUyCLoC6xotFwFHN7HdeWY2BigAbnTOrdt7AzO7ArgCoHv37iEoVaRpCXE+Hv/ekZzxxw859/E5lAfqaJsSzy3jc/n+sT1pk+Cn3jkefLOAY3q3Y2SPdl6XLBGkMgY6i5s6T9773v1XgJ7OuaHAW8Bfm/og59wU51yecy4vOzu7hcsU2b+O6Uk8cdGR9GjfhlvH9+fDW7/FT8b2ITUxDjPjt+cOoUtmEtc9v4hdlRrBVJpvz1VD4TwfwWEqAro1Ws4BNjTewDm3zTlXHVx8ChgZwnpEDtnIHu2Yef1ofjz2CNokfv0fbXpSPH+cfCTFJVXcNn2JxiqSZqsM9hF43TQUyiCYB/Q1s15mlgBMAmY03sDMOjdaPBNYHsJ6REJmeLdMbj41l9e/2MRzn37ldTkSIaL+PgLnXC1wDTCLhi/4F5xzS83sLjM7M7jZdWa21MwWA9cBl4SqHpFQu2J0b0b3zeKuV5ZpHmRplopYuLPYOTfTOdfPOXeEc+7u4LrbnXMzgs9/5pwb5Jwb5pw7yTm3IpT1iISSz2c8eP5w0pLiuejpT3jozQKKdlR4XZaEscqaOpLiffg8vvRYdxaLtKDstET+ckke/Tul8cg7Kxl932wufvoTXlm8QSOayjdUBGo9v5kMQnv5qEhMGpqTyd8vO5qiHRW8OL+IlxYUce3zC2nfJoG/X3Y0A7uke12ihImKQJ3nzUKgMwKRkMlpm8KNJ/fj/VtO4u+XjcLvM66ftpCqGp0ZSIPKMJidDBQEIiHn9xmj+2bzwMRhrNxcxr2vqytMGoTDNJWgIBBpNWP6ZXPp8T2ZOncN7xVs8bocCQPhME0lKAhEWtWt4/vTr2MqN7+4mG1l1Qd+g0S1dTsq6JyR7HUZCgKR1pQU7+cPF4xgV0UNP/vX57oLOYaVVNWwcVcV/TqmeV2KgkCktQ3sks5PT83ljWXF/HPeN8ZYlBixsrjhpsPcTqkeV6LLR0U8cdkJvZidv5k7X1lGTb2jKlDHrsqaPY+ubZO5YVxfEuO8bz+W0MjfVAZA3w7enxEoCEQ84PMZvz9/GGc88iG/+s8XDesMMpLjSU+OZ8biDSz6aidPXDySjOR4j6uVUCgoLqVNgp+umd73ESgIRDzSOSOZ9245iR3lATJS4klNiNsz1MC/FxZxy0tLmPjEXKZeOoouYfBlIS2roLiUvh3TPB9eAtRHIOKp1MQ4urVLIT0p/mtfCOeMyOGvl45i484qznl8Dss2lHhYpYRCQXEp/Tp63z8ACgKRsHVcnyxe/PGx+Mw4/8mP+GCl7j2IFtvKqtlaFgiLK4ZAQSAS1vp3SudfPzmOnLbJXPrMPD5dvd3rkqQFFBQ3dBTndlIQiEgzdM5I5oWrjqVzZhK3TV+isYqiQEHw0lGdEYhIs6UnxXPPOUNZtbWcR95e6XU5cpjyi0vJSI6nQ1qi16UACgKRiHFC3ywmjszhyfdX8cX6XV6XI4dhZXEpuR3TMPP+iiFQEIhElF+ePpB2bRK4dfoSauvqvS5HDoFzjvxNpfQLgzuKd1MQiESQjJR4fnPWIJZuKOGpD1Z7XY4cguKSakqqasOmfwAUBCIRZ/zgzowf1ImH3ipg1ZYyr8uRg5QfZh3FoCAQiUh3nTWIpDgft03/nPp6jWAaSVYqCESkJXRIT+KXZwzk0zXbmfLBKq/LkYOQv6mUrNRE2rVJ8LqUPTTWkEiEmjgyh3eWb+be11dQW1fP1Sf1CZurUGTfCopLw2Lo6cZ0RiASocyMP144gnNGdOWBNwr47czlmugmzNXXO1ZuLgurZiHQGYFIRIv3+/j9xGGkJ8Xx1Aer2VVZw2/PGUKcX3/jhaP1OyupCNQpCESkZfl8xh1nDiIjJYFH3l5JSWUtD08erkltwlD+pvDrKAY1DYlEBTPjppP7cfsZA/nv0k1cNnW+xiQKQ/+7dFR9BCISIj88oRf3TxjKnC+3cuXfF1Bd2zJhUFtXz2tLNlJSVdMinxerVhaX0iUjibSk8Jp1TkEgEmUm5nXjnnOG8F7BFq5+diE1LTAUxdMfrubq5z5j/EPvM7dwawtUGZvyi8voFyZDTzemIBCJQpNGdeeuswbx1vJibpi26LDGJSraUcEf3lrJqF7tSIr3c+GfP+HOV5aq6ekg1dbV8+XmMnLDrH8A1FksErW+f2xPArX1/N9ry4n3G78/fzj+g5wf1znHr19eihk8dMFw2qUkcO/ry3lmzhreL9jCQxcMZ2hOZoh+guiydnsFgbp6+ioIRKQ1/Wh0b6pr67l/Vj4+M0b3y2JbWYDt5QG2lQXYURHg1EGdOG9kTpPvn7W0mLdXbOYXpw2ga2YyAHeeNZhxAzvy0xeXcM7jc/n5aQO47IRerfljRaSC4BVDOiMQkVZ39Ul9qK6p45F3CvnXwvUAxPmMdm0SiPMZbywrZv3OSq791tfvTC6rruWOGUvp3ymNS47v+bXPHN03m1k3jOGW6Yv5zavLKKuq5bpv687m/ckvLsUM+nQIryuGQEEgEhNuPLkfZw7vis+gfZtE0pPjMDNq6uq59aUlPPhmAdvLA9x+xkB8weajh94soLi0iscvOpL4Jm5Qy0iJ5/HvjeSWl5bw0FsFVNXWccupuQqDfSgoLqV7uxSSE8Lv/g4FgUgMMLMm/xKN9/t4YOIw2rZJ4OkPV7OjIsD9E4ZRUFzKM3NWc+Go7hzZve0+P9fvM+6fMJTEeB9/evdLqmrquP2MgQqDJhQUh9/QErspCERinM9n/PL0AbRPTeC+/+azs6KGnRUB2rVJ4Jbx/Zv1/rvPHkxinI9n5qyhqqaeu88evOfMQqC6to7VW8sZP6iT16U0SUEgIpgZPxnbh3YpCfz8359T7+DhScPJSG7ejU9mxu1nDCQ53s/j735JTV09908YqjODoE9Wbaeu3jE0J8PrUpqkIBCRPSaN6k7H9CQ+X7+LM4d1Oaj3mhk/PTWXOL+PR95eSe/sNvxkbJ8QVRpZ/rNoPWlJcYzpl+11KU1SEIjI15zUvwMn9e9wSO81M24c15fVW8u5f1Y+AzunMzb30D4rWlQG6pj1xSbOGNqFpPjw6yiGEN9ZbGbjzSzfzArN7Lb9bDfBzJyZ5YWyHhEJPTPjd+cNIbdjGtc9v5C128q9LslTby0vpjxQx1kjDu4MqzWFLAjMzA88BnwHGAhMNrOBTWyXBlwHfBKqWkSkdaUkxDHl4jzMjCv+toDy6lqvS/LMfxaup3NGEsf0au91KfsUyjOCUUChc26Vcy4ATAPOamK73wD3AVUhrEVEWln39ik8euEIVm4u5ZbpS742e9q67RX8/o18Rt/3DhP+NDdqB7LbXh7gvYItnDmsS1hfRRXKPoKuwLpGy0XA0Y03MLMRQDfn3KtmdvO+PsjMrgCuAOjevXsIShWRUBjdN5tbxvfn3tdXMKBTGt3bt+GFeev4sHArZnBCnywKN5dx4Z8/4bgj2nPzqbn7vW8h0rz2+UZq6x1nDe/qdSn7FcogaCr+9vxJYGY+4CHgkgN9kHNuCjAFIC8vT5OyikSQK8f05ov1u3jgjQIAumYmc+O4fkzIy6FrZjJVNXU898lXPDa7kHMfn8u4AR24+dRc+ndK97jyw/fywvX065jKgM7heSPZbqEMgiKgW6PlHGBDo+U0YDDwbvBa407ADDM70zk3P4R1iUgrMjPumzCUvh3SGNmjLccd0f5rzSRJ8X5+eEIvLjiqG1PnruHJ977k7Mfm8NZNJ5LTNsXDyg/Puu0VzF+7g59GwLAboewjmAf0NbNeZpYATAJm7H7RObfLOZflnOvpnOsJfAwoBESiUEpCHNeP68sJfbP22VbeJjGOq0/qw8zrRwNwz+srWrPEFjdjccPfvWcND9+rhXYLWRA452qBa4BZwHLgBefcUjO7y8zODNV+RSSy5bRN4aoTj+C1JRv5ZNU2r8s5JM45/r1wPaN6touIs5qQ3kfgnJvpnOvnnDvCOXd3cN3tzrkZTWw7VmcDIgJw5Zgj6JKRxF2vLqOuPvK6BZduKKFwc1lY3zvQmKaqFJGwk5zg57bTBrB0Qwkvzl934DeEmZcXrSfeb5w+pLPXpTSLgkBEwtJ3h3Ymr0dbHngjn5KqGq/Laba6eseMxbG58hoAAAoxSURBVBs4sV8HMlMSvC6nWTTWkIiEJTPj198dxJmPfcij7xTy89MGeFZLcUkV33/6UxyOrNRE2qcmkpWaQFZqIolxPqpr6wnU1hOoq2dLaTXFJdX86ozIaBYCBYGIhLEhORlMHJnDM3NWM3lUd3pltfGkjntfX8HqreWc1D+brWUBlhTtZGtpNeWBuq9tF+83Evw++ndKY9yAjp7UeigUBCIS1m4+NZeZn2/i7teW8ecfHNXq+1+wdgf/Xrien4w94hsT9VQG6gjU1ZMY5yPB7wvrYST2R30EIhLWOqQlcc23+vDW8s08/eHqr41ZFGr19Y47X1lKx/RErj7pm3MrJCf4yUiOJyneH7EhAAoCEYkAlx7fk3EDOvKbV5dx6/QlVNXUHfhNLeClBUUsKdrFz74zgDaJ0duAoiAQkbCXGOdnysUjue5bfXhhfhGTpnxMcUloBywuqarhvlkrGNmjbUTcHXw4FAQiEhF8PuOmU3J54qIjKSgu5Yw/fsiCtTsO6zPXba9g3faKJl/749sr2VYe4I7vDgr7sYIOl4JARCLK+MGd+fdPjic53s+kKR/xwiHecFZWXcs5j8/lxPtnc/20hRQUl+55rXBzGc/MWcMFed0YEqYTzrckBYGIRJzcTmnMuOZ4ju7VnlteWsLDb6086E7kKe99ydayas49Moc3lxVzykPvc8Xf5rOkaCe/eXUZyQl+bj41N0Q/QXiJ3t4PEYlqmSkJPHPpUdw6fQkPvVXAppJKfnPWYOL8B/77trikiqc+WM3pQzvzwMRh/OK0ATwzZzVT567hjWXFAPzy9AFkpSaG+scICwoCEYlY8X4fv584jC4ZyTw6u5DikmoevXAEKQn7/2p78I0CauvrufXUhvsC2rZJ4KZTcrl8TG/+/vFa1mwt5wfH9WyFnyA8KAhEJKKZGTefmkunjCRuf/kLJk/5mKcvOWqff82v2FTCCwvWcelxveje/utDRKclxfOTsd+8XyDaqY9ARKLCRcf04MmL88gvLuW8P839WudvY/fMXEFaYhzXfiv2vvD3RUEgIlHj5IEdee7yY6gI1HH2Y3N4/fONX3v9w5Vbea9gC9d8qw9t20TGyKCtQUEgIlHlyO5tefXaE8jtlMaPn/2M+2etoK7eUVfvuHvmcrpmJvP9Y3t6XWZYUR+BiESdjulJTLviGO6YsZTHZn/J0g0ljOmbzfKNJTw8aThJ8X6vSwwrCgIRiUqJcX7uOXcoQ7pm8usZX/Bu/haG5mTw3aHRPVzEoVAQiEhUu/Do7uR2SuN3r6/g56cPiOhRQkNFQSAiUW9kj7a8cNWxXpcRttRZLCIS4xQEIiIxTkEgIhLjFAQiIjFOQSAiEuMUBCIiMU5BICIS4xQEIiIxzg52ejevmdkWYO1+NskAdh3gY/a3zb5ea2r93uv2t5wFbD1AXYeiOT/vobzHi2ME4XOcmrv9wR6nQ1kXrseoue9pid8lHaPmrd/fcevhnMtucg/Ouah6AFMOZ5t9vdbU+r3X7W8ZmO/VzxspxyicjlNztz/Y43Qo68L1GDX3PS3xu6RjdGi/X82tNxqbhl45zG329VpT6/ded6DlUDiUfcTaMTqU/TR3+4M9ToeyLlyPUXPf0xK/SzpGzVt/SP++Iq5pKFKZ2XznXJ7XdYQ7HacD0zE6MB2jgxONZwThaorXBUQIHacD0zE6MB2jg6AzAhGRGKczAhGRGKcgEBGJcQoCEZEYpyAIE2bWxswWmNkZXtcSjsxsgJk9YWYvmdmPva4nHJnZ2Wb2lJm9bGaneF1PuDKz3mb2tJm95HUt4UJBcJjM7C9mttnMvthr/XgzyzezQjO7rRkfdSvwQmiq9FZLHCPn3HLn3FXA+UDUXRbYQsfoP865y4FLgAtCWK5nWug4rXLOXRbaSiOLrho6TGY2BigD/uacGxxc5wcKgJOBImAeMBnwA/fs9RE/BIbScEt8ErDVOfdq61TfOlriGDnnNpvZmcBtwKPOuedaq/7W0FLHKPi+3wPPOuc+a6XyW00LH6eXnHMTWqv2cKbJ6w+Tc+59M+u51+pRQKFzbhWAmU0DznLO3QN8o+nHzE4C2gADgUozm+mcqw9p4a2oJY5R8HNmADPM7DUgqoKghX6PDLgXeD0aQwBa7ndJvk5BEBpdgXWNlouAo/e1sXPuFwBmdgkNZwRREwL7cVDHyMzGAucCicDMkFYWPg7qGAHXAuOADDPr45x7IpTFhZGD/V1qD9wNjDCznwUDI6YpCELDmlh3wDY459zUli8lbB3UMXLOvQu8G6piwtTBHqNHgEdCV07YOtjjtA24KnTlRB51FodGEdCt0XIOsMGjWsKVjtGB6Rg1j47TYVIQhMY8oK+Z9TKzBGASMMPjmsKNjtGB6Rg1j47TYVIQHCYzex74CMg1syIzu8w5VwtcA8wClgMvOOeWelmnl3SMDkzHqHl0nEJDl4+KiMQ4nRGIiMQ4BYGISIxTEIiIxDgFgYhIjFMQiIjEOAWBiEiMUxBI1DCzslbe35/NbGAr7/MGM0tpzX1K9NN9BBI1zKzMOZfagp8XF7xZqdUERxC1fQ08aGZrgDzn3NbWrEuim84IJKqZWbaZTTezecHH8cH1o8xsrpktDP43N7j+EjN70cxeAd4ws7Fm9m5wZrQVZvZs8Mua4Pq84PMyM7vbzBab2cdm1jG4/ojg8jwzu6upsxYz62lmy83sceAzoJuZ/cnM5pvZUjO7M7jddUAXYLaZzQ6uO8XMPjKzz4J1t1gQSgxxzumhR1Q8gLIm1j0HnBB83h1YHnyeDsQFn48DpgefX0LDIGbtgstjgV00DGTmo2F4g92f9y4Nf51Dw2iX3w0+vw/4ZfD5q8Dk4POr9lFjT6AeOKbRut379wf3MzS4vAbICj7PAt4H2gSXbwVu9/r/gx6R99Aw1BLtxgEDg3/EA6SbWRqQAfzVzPrS8CUe3+g9bzrntjda/tQ5VwRgZoto+OL+cK/9BGj40gdYQMNsWQDHAmcHnz8HPLCPOtc65z5utHy+mV1Bw1DxnWmYtGjJXu85Jrh+TvDnS6AhqEQOioJAop0PONY5V9l4pZn9EZjtnDsnOOPVu41eLt/rM6obPa+j6X83Nc45d4Bt9mfPPs2sF3AzcJRzboeZTaVhGtO9GQ2hNfkg9yXyNeojkGj3Bg0jUwJgZsODTzOA9cHnl4Rw/x8D5wWfT2rme9JpCIZdwb6G7zR6rRRIa/TZx5tZHwAzSzGzfodfssQaBYFEk5Tg0MS7HzcB1wF5ZrbEzJbxv5mp7gPuMbM5NLTDh8oNwE1m9ikNTTy7DvQG59xiYCGwFPgLMKfRy1OA181stnNuCw0h9ryZLaEhGPq3bPkSC3T5qEgIBa/5r3TOOTObREPH8Vle1yXSmPoIREJrJPBo8JLTncAPPa5H5Bt0RiAiEuPURyAiEuMUBCIiMU5BICIS4xQEIiIxTkEgIhLjFAQiIjHu/wPMtVUu7CS2ugAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "lr_finder = LRFinder(model, optimizer, criterion, device=\"cuda\")\n",
    "lr_finder.range_test(data_loader_train, end_lr=100, num_iter=100)\n",
    "lr_finder.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "#i = 0\n",
    "#train_csv_file = pd.read_csv(f'/mnt/disks/mount_point/train_splitted/train{i}.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_model(i):\n",
    "    \n",
    "    train_dataset = IntracranialDataset(\n",
    "        csv_file=f'/mnt/disks/mount_point/train_splitted/train{i}.csv', path='/mnt/disks/mount_point/stage_1_train_images_jpg', transform=transform_train, labels=True)\n",
    "    print(f'/mnt/disks/mount_point/train_splitted/train{i}.csv')\n",
    "    data_loader_train = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "    since = time.time()\n",
    "    \n",
    "    #checkpoint = torch.load('/mnt/disks/mount_point/train{i-1}.pth')\n",
    "    #model.load_state_dict(checkpoint['model_state_dict'])\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, n_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            if phase == 'train':\n",
    "                dataloader = data_loader_train\n",
    "            else:\n",
    "                dataloader = data_loader_valid\n",
    "            for inputs, labels in dataloader:\n",
    "                inputs = inputs.to(device, dtype=torch.float)\n",
    "                labels = labels.to(device, dtype=torch.float)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    #_, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                #running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            if phase == 'train':\n",
    "                dataset_size = len(train_dataset)\n",
    "            else:\n",
    "                dataset_size = len(valid_dataset)\n",
    "            epoch_loss = running_loss / dataset_size\n",
    "            #epoch_acc = running_corrects.double() / dataset_size\n",
    "\n",
    "            #print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "             #   phase, epoch_loss, epoch_acc))\n",
    "            print(f'Loss: {epoch_loss}')\n",
    "\n",
    "            # deep copy the model\n",
    "            #if phase == 'val' :\n",
    "             #   val_acc = epoch_acc\n",
    "              #  print(f'val_acc: {val_acc}')\n",
    "                \n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    torch.save(model.state_dict(), f'/mnt/disks/mount_point/train{i}.pth')\n",
    "    return\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/disks/mount_point/train_splitted/train0.csv\n",
      "Epoch 0/0\n",
      "----------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-b90eb5fd7054>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtraining_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-12-03e70806e4d8>\u001b[0m in \u001b[0;36mtraining_model\u001b[0;34m(i)\u001b[0m\n\u001b[1;32m     45\u001b[0m                     \u001b[0;31m# backward + optimize only if in training phase\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                     \u001b[0;32mif\u001b[0m \u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph)\u001b[0m\n\u001b[1;32m    116\u001b[0m                 \u001b[0mproducts\u001b[0m\u001b[0;34m.\u001b[0m \u001b[0mDefaults\u001b[0m \u001b[0mto\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;31m`\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m         \"\"\"\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables)\u001b[0m\n\u001b[1;32m     91\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m     92\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 93\u001b[0;31m         allow_unreachable=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for i in range(1):\n",
    "    training_model(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook ongoing.ipynb to python\n",
      "[NbConvertApp] Writing 6677 bytes to ongoing.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to python ongoing.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "for i in range(1, 11):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train0.csv   train14.csv  train19.csv  train23.csv  train3.csv  train8.csv\n",
      "train10.csv  train15.csv  train1.csv   train24.csv  train4.csv  train9.csv\n",
      "train11.csv  train16.csv  train20.csv  train25.csv  train5.csv  valid.csv\n",
      "train12.csv  train17.csv  train21.csv  train26.csv  train6.csv\n",
      "train13.csv  train18.csv  train22.csv  train2.csv   train7.csv\n"
     ]
    }
   ],
   "source": [
    "ls /mnt/disks/mount_point/train_splitted/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mv /mnt/disks/mount_point/train_splitted/df6 /mnt/disks/mount_point/train_splitted/train24.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[0m\u001b[01;34mbackups\u001b[0m/                           test.csv\n",
      "\u001b[01;34mcheckpoints\u001b[0m/                       train0.csv\n",
      "\u001b[01;34mcopied_from_bootd\u001b[0m/                 train12.csv\n",
      "example.pth                        train16.csv\n",
      "ipython_log.py                     train20.csv\n",
      "\u001b[01;34mlost+found\u001b[0m/                        train20.pth\n",
      "ongoing.ipynb                      train21.pth\n",
      "process_csv.py                     train22.pth\n",
      "rsna.ipynb                         train23.pth\n",
      "rsna.py                            train24.csv\n",
      "rsna_resnext.py                    train24.pth\n",
      "sample.txt                         train25.pth\n",
      "sometext.txt                       train26.pth\n",
      "stage_1_sample_submission.csv      train4.csv\n",
      "stage_1_sample_submission.csv.zip  train8.csv\n",
      "\u001b[01;34mstage_1_test_images_jpg\u001b[0m/           train.csv\n",
      "stage_1_train.csv                  \u001b[01;34mtrain_splitted\u001b[0m/\n",
      "stage_1_train.csv.zip              trial.py\n",
      "\u001b[01;34mstage_1_train_images_jpg\u001b[0m/          Untitled.ipynb\n",
      "stderr.txt                         Untitled.py\n",
      "stdout.txt                         valid.csv\n"
     ]
    }
   ],
   "source": [
    "ls /mnt/disks/mount_point/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "!cp /mnt/disks/mount_point/train0.pth /mnt/disks/mount_point/backups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID_a81d35352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID_6d4929f62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID_8e1f5e5ee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID_c5f68d242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID_8b05d75f6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134847</td>\n",
       "      <td>ID_518c0608a</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134848</td>\n",
       "      <td>ID_b3debf4a3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134849</td>\n",
       "      <td>ID_5fa900312</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134850</td>\n",
       "      <td>ID_aabd4137e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134851</td>\n",
       "      <td>ID_26370f428</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>134852 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Image  any  epidural  intraparenchymal  intraventricular  \\\n",
       "0       ID_a81d35352    0         0                 0                 0   \n",
       "1       ID_6d4929f62    1         0                 0                 0   \n",
       "2       ID_8e1f5e5ee    0         0                 0                 0   \n",
       "3       ID_c5f68d242    0         0                 0                 0   \n",
       "4       ID_8b05d75f6    0         0                 0                 0   \n",
       "...              ...  ...       ...               ...               ...   \n",
       "134847  ID_518c0608a    0         0                 0                 0   \n",
       "134848  ID_b3debf4a3    0         0                 0                 0   \n",
       "134849  ID_5fa900312    0         0                 0                 0   \n",
       "134850  ID_aabd4137e    0         0                 0                 0   \n",
       "134851  ID_26370f428    0         0                 0                 0   \n",
       "\n",
       "        subarachnoid  subdural  \n",
       "0                  0         0  \n",
       "1                  0         1  \n",
       "2                  0         0  \n",
       "3                  0         0  \n",
       "4                  0         0  \n",
       "...              ...       ...  \n",
       "134847             0         0  \n",
       "134848             0         0  \n",
       "134849             0         0  \n",
       "134850             0         0  \n",
       "134851             0         0  \n",
       "\n",
       "[134852 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/mnt/disks/mount_point/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr0 = pd.read_csv('/mnt/disks/mount_point/train0.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID_96492def9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID_2bb5b7cb8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID_626443765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID_3496f44f7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID_0504f4738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19995</td>\n",
       "      <td>ID_b93ad71b7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19996</td>\n",
       "      <td>ID_d8eb2cd68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19997</td>\n",
       "      <td>ID_aefe6fa3d</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19998</td>\n",
       "      <td>ID_a3f66447d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999</td>\n",
       "      <td>ID_8a8afe886</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Image  any  epidural  intraparenchymal  intraventricular  \\\n",
       "0      ID_96492def9    0         0                 0                 0   \n",
       "1      ID_2bb5b7cb8    1         1                 0                 0   \n",
       "2      ID_626443765    0         0                 0                 0   \n",
       "3      ID_3496f44f7    1         0                 0                 0   \n",
       "4      ID_0504f4738    0         0                 0                 0   \n",
       "...             ...  ...       ...               ...               ...   \n",
       "19995  ID_b93ad71b7    0         0                 0                 0   \n",
       "19996  ID_d8eb2cd68    0         0                 0                 0   \n",
       "19997  ID_aefe6fa3d    1         0                 0                 0   \n",
       "19998  ID_a3f66447d    0         0                 0                 0   \n",
       "19999  ID_8a8afe886    0         0                 0                 0   \n",
       "\n",
       "       subarachnoid  subdural  \n",
       "0                 0         0  \n",
       "1                 0         0  \n",
       "2                 0         0  \n",
       "3                 0         1  \n",
       "4                 0         0  \n",
       "...             ...       ...  \n",
       "19995             0         0  \n",
       "19996             0         0  \n",
       "19997             1         0  \n",
       "19998             0         0  \n",
       "19999             0         0  \n",
       "\n",
       "[20000 rows x 7 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tr0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr4 = pd.read_csv('/mnt/disks/mount_point/train4.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr8 = pd.read_csv('/mnt/disks/mount_point/train8.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr12 = pd.read_csv('/mnt/disks/mount_point/train12.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr16 = pd.read_csv('/mnt/disks/mount_point/train16.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr20 = pd.read_csv('/mnt/disks/mount_point/train20.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr24 = pd.read_csv('/mnt/disks/mount_point/train24.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "newvalid = pd.concat([tr0, tr4, tr8, tr12, tr16, tr20, tr24])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "newvalid.to_csv('/mnt/disks/mount_point/valid.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID_96492def9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID_2bb5b7cb8</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID_626443765</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID_3496f44f7</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID_0504f4738</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139995</td>\n",
       "      <td>ID_6a1a5065e</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139996</td>\n",
       "      <td>ID_57c54b888</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139997</td>\n",
       "      <td>ID_38213fe3e</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139998</td>\n",
       "      <td>ID_7087cbd7d</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139999</td>\n",
       "      <td>ID_e50429606</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>140000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "               Image  any  epidural  intraparenchymal  intraventricular  \\\n",
       "0       ID_96492def9    0         0                 0                 0   \n",
       "1       ID_2bb5b7cb8    1         1                 0                 0   \n",
       "2       ID_626443765    0         0                 0                 0   \n",
       "3       ID_3496f44f7    1         0                 0                 0   \n",
       "4       ID_0504f4738    0         0                 0                 0   \n",
       "...              ...  ...       ...               ...               ...   \n",
       "139995  ID_6a1a5065e    1         0                 0                 0   \n",
       "139996  ID_57c54b888    0         0                 0                 0   \n",
       "139997  ID_38213fe3e    0         0                 0                 0   \n",
       "139998  ID_7087cbd7d    0         0                 0                 0   \n",
       "139999  ID_e50429606    1         0                 0                 1   \n",
       "\n",
       "        subarachnoid  subdural  \n",
       "0                  0         0  \n",
       "1                  0         0  \n",
       "2                  0         0  \n",
       "3                  0         1  \n",
       "4                  0         0  \n",
       "...              ...       ...  \n",
       "139995             1         0  \n",
       "139996             0         0  \n",
       "139997             0         0  \n",
       "139998             0         0  \n",
       "139999             0         0  \n",
       "\n",
       "[140000 rows x 7 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/mnt/disks/mount_point/valid.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: remove write-protected regular file '/mnt/disks/mount_point/train20.pth'? ^C\n"
     ]
    }
   ],
   "source": [
    "!rm /mnt/disks/mount_point/train2?.pth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = pd.read_csv('/mnt/disks/mount_point/train_splitted/valid.csv', chunksize=20000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, df in enumerate(reader):\n",
    "    df.to_csv(f'/mnt/disks/mount_point/train_splitted/df{i}', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Image</th>\n",
       "      <th>any</th>\n",
       "      <th>epidural</th>\n",
       "      <th>intraparenchymal</th>\n",
       "      <th>intraventricular</th>\n",
       "      <th>subarachnoid</th>\n",
       "      <th>subdural</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>ID_a81d35352</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>ID_6d4929f62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>ID_8e1f5e5ee</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>ID_c5f68d242</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>ID_8b05d75f6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19995</td>\n",
       "      <td>ID_f4f2fb9eb</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19996</td>\n",
       "      <td>ID_03f9c4cfc</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19997</td>\n",
       "      <td>ID_76b739440</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19998</td>\n",
       "      <td>ID_3e9600341</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19999</td>\n",
       "      <td>ID_04ede1da4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>20000 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "              Image  any  epidural  intraparenchymal  intraventricular  \\\n",
       "0      ID_a81d35352    0         0                 0                 0   \n",
       "1      ID_6d4929f62    1         0                 0                 0   \n",
       "2      ID_8e1f5e5ee    0         0                 0                 0   \n",
       "3      ID_c5f68d242    0         0                 0                 0   \n",
       "4      ID_8b05d75f6    0         0                 0                 0   \n",
       "...             ...  ...       ...               ...               ...   \n",
       "19995  ID_f4f2fb9eb    1         0                 0                 0   \n",
       "19996  ID_03f9c4cfc    0         0                 0                 0   \n",
       "19997  ID_76b739440    0         0                 0                 0   \n",
       "19998  ID_3e9600341    0         0                 0                 0   \n",
       "19999  ID_04ede1da4    0         0                 0                 0   \n",
       "\n",
       "       subarachnoid  subdural  \n",
       "0                 0         0  \n",
       "1                 0         1  \n",
       "2                 0         0  \n",
       "3                 0         0  \n",
       "4                 0         0  \n",
       "...             ...       ...  \n",
       "19995             1         0  \n",
       "19996             0         0  \n",
       "19997             0         0  \n",
       "19998             0         0  \n",
       "19999             0         0  \n",
       "\n",
       "[20000 rows x 7 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.read_csv('/mnt/disks/mount_point/train_splitted/df0')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
